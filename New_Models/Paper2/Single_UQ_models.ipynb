{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forestci is an optional dependency. To install latest forestci compatabilty with scikit-learn>=0.24, run pip install git+git://github.com/scikit-learn-contrib/forest-confidence-interval.git\n",
      "XGBoost is an optional dependency. If you want to use XGBoost models, please manually install xgboost package with pip install xgboost. If have error with finding libxgboost.dylib library, dobrew install libomp. If do not have brew on your system, first do ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" from the Terminal\n",
      "scikit-lego is an optional dependency, enabling use of the LowessRegression model. If you want to use this model, do \"pip install scikit-lego\"\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''First, we need to define the path of where to get the dataset, and define other parameters that we will need'''\n",
    "import sys\n",
    "sys.path.append('/Users/jakehirst/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models')\n",
    "\n",
    "from NN_fed_GPR import *\n",
    "from NN_fed_RF import *\n",
    "from RF_fed_GPR import *\n",
    "from Bagging_models import *\n",
    "from Backward_feature_selection import *\n",
    "import ast\n",
    "\n",
    "model_types = ['Single RF', 'Single GPR', 'NN_fed_GPR', 'NN_fed_RF', 'RF_fed_GPR']\n",
    "model_types = ['NN_fed_RF', 'RF_fed_GPR']\n",
    "\n",
    "# model_types = ['Single GPR']\n",
    "\n",
    "all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "labels_to_predict = ['impact site x', 'impact site y', 'height']\n",
    "# labels_to_predict = ['height']\n",
    "\n",
    "with_or_without_transformations = 'with'\n",
    "with_or_without_transformations = 'without'\n",
    "\n",
    "Paper2_path = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations'\n",
    "model_folder = Paper2_path + f'/UQ_bagging_models_{with_or_without_transformations}_transformations'\n",
    "data_folder = Paper2_path + '/5fold_datasets'\n",
    "results_folder = Paper2_path + '/Compare_Code_5_fold_ensemble_results'\n",
    "hyperparam_folder = Paper2_path + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "\n",
    "image_folder = '/Users/jakehirst/Desktop/sfx/sfx_ML_data/images_sfx/new_dataset/Visible_cracks'\n",
    "\n",
    "if(with_or_without_transformations == 'with'):\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/dataset/feature_transformations_2023-11-16/height/HEIGHTALL_TRANSFORMED_FEATURES.csv\"\n",
    "    backward_feat_selection_results_folder = '/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/results'\n",
    "else:\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_no_feature_engineering/dataset/New_Crack_Len_FULL_OG_dataframe_2023_11_16.csv\"\n",
    "    backward_feat_selection_results_folder = Paper2_path + '/Paper_2_results_WITHOUT_feature_engineering/results' \n",
    "    df = pd.read_csv(full_dataset_pathname, index_col=0)\n",
    "    all_features = df.columns\n",
    "    all_features = all_features.drop(all_labels)\n",
    "    all_features = str(all_features.drop('timestep_init').to_list())\n",
    "\n",
    "    print(all_features)\n",
    "    \n",
    "    \n",
    "'''Only have to uncomment this if the 5 fold datasets have not been made or need to be remade'''\n",
    "# make_5_fold_datasets(data_folder, full_dataset_pathname, image_folder)\n",
    "\n",
    "print('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using just the basic features\n",
      "using just the basic features\n",
      "using just the basic features\n",
      "{'impact site x': {'RF_fed_GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}, 'impact site y': {'RF_fed_GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}, 'height': {'RF_fed_GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}}\n"
     ]
    }
   ],
   "source": [
    "'''get the appropriate features that each model will use based on backward feature elimination'''\n",
    "all_features_to_keep = {}\n",
    "\n",
    "min_features = 1 #minimum number of features you want to select from BFS (backward feature selection)\n",
    "max_features = 25 #maximum number of features you want to select from BFS\n",
    "for label in labels_to_predict:\n",
    "    all_features_to_keep[label] = {}\n",
    "    for model_type in model_types:\n",
    "        \n",
    "        if('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname):\n",
    "            print('true')\n",
    "            model_type_hyperparam = model_type.removeprefix('Single ')\n",
    "            #TODO use code below if using feature selection\n",
    "            best_features = get_best_features(backward_feat_selection_results_folder, label, model_type_hyperparam, min_features, max_features)\n",
    "            all_features_to_keep[label][model_type] = best_features\n",
    "        \n",
    "        else:\n",
    "            print('using just the basic features')\n",
    "            #TODO use code below if NOT using feature selection\n",
    "            all_features_to_keep[label][model_type] = all_features\n",
    "\n",
    "print(all_features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site x/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 21.3**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.408)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site y/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 18.2**2 * Matern(length_scale=10, nu=1.5) + WhiteKernel(noise_level=0.558)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/height/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 0.828**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.102)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site x/fold2/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 22.3**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=1.13)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site y/fold2/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 18.2**2 * Matern(length_scale=10, nu=1.5) + WhiteKernel(noise_level=0.559)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/height/fold2/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 0.837**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.123)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site x/fold3/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 21.8**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.409)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site y/fold3/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 18.2**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.65)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/height/fold3/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 0.845**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.102)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site x/fold4/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 21.7**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=1.02)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site y/fold4/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 18.4**2 * Matern(length_scale=10.3, nu=1.5) + WhiteKernel(noise_level=0.619)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/height/fold4/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 0.871**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.119)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site x/fold5/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 21.3**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=1.26)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/impact site y/fold5/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 19**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.41)\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/without_transformations/5fold_datasets/height/fold5/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "OPTIMIZED GPR PARAMETERS FOR NN-->GPR = 0.819**2 * Matern(length_scale=100, nu=1.5) + WhiteKernel(noise_level=0.0907)\n"
     ]
    }
   ],
   "source": [
    "'''Now we will make all of the models'''\n",
    "\n",
    "\n",
    "def make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, num_training_points=False, model_type=None): \n",
    "    models = []\n",
    "    training_features = training_features[features_to_keep]\n",
    "    current_label = training_labels.columns[0]\n",
    "    if(not os.path.exists(model_saving_folder)): os.mkdir(model_saving_folder)\n",
    "\n",
    "    if(model_type == 'Single RF'):\n",
    "        depth, features, samples_leaf, samples_split, estimators = get_best_hyperparameters_RF(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model =  RandomForestRegressor(max_depth=depth, max_features=features, \n",
    "                                       min_samples_leaf = samples_leaf, min_samples_split = samples_split, n_estimators=estimators, random_state=42)\n",
    "        model.fit(training_features, training_labels)\n",
    "        \n",
    "    elif(model_type == 'Single GPR'):\n",
    "        c, length_scale, noise_level = get_best_hyperparameters_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        kernel = ConstantKernel(constant_value=c) * RBF(length_scale=length_scale) + WhiteKernel(noise_level=noise_level)\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=0, n_restarts_optimizer=25)\n",
    "        model.fit(training_features, training_labels)\n",
    "        \n",
    "    elif(model_type == 'NN_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "\n",
    "    elif(model_type == 'RF_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = RF_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "        \n",
    "    elif(model_type == 'NN_fed_RF'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_RF()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder, num_optimization_tries=100, hyperparam_folder=f'/Volumes/Jake_ssd/Paper 2/without_transformations/optimized_hyperparams/NN_fed_RF/{current_label}')\n",
    "        \n",
    "    save_ensemble_model(model, 1, model_saving_folder) \n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "for fold_no in range(1,6):\n",
    "    for model_type in model_types:\n",
    "        for label_to_predict in labels_to_predict:\n",
    "            print(f'\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting {label_to_predict} using {model_type} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n')\n",
    "            \n",
    "            all_labels = ['height', 'phi', 'theta', \n",
    "                        'impact site x', 'impact site y', 'impact site z', \n",
    "                        'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "            print(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv')\n",
    "            training_features = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv').reset_index(drop=True)\n",
    "            training_labels = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_labels.csv').reset_index(drop=True)\n",
    "\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(model_saving_folder)):\n",
    "                os.makedirs(model_saving_folder)\n",
    "                \n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(results_saving_folder)):\n",
    "                os.makedirs(results_saving_folder)\n",
    "            # make_dirs(model_saving_folder)\n",
    "            # make_dirs(results_saving_folder)\n",
    "\n",
    "            '''TODO gotta find out what features to use for each label before testing on new dataset'''\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            print(features_to_keep)\n",
    "            make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, 1, features_to_keep, hyperparam_folder, model_type=model_type)\n",
    "            # make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE = RF_fed_GPR\n",
      "LABEL = impact site x\n",
      "fold 1\n",
      "Calibration error = 1.77\n",
      "fold 2\n",
      "Calibration error = 0.37\n",
      "fold 3\n",
      "Calibration error = 1.27\n",
      "fold 4\n",
      "Calibration error = 0.93\n",
      "fold 5\n",
      "Calibration error = 1.66\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 0.42\n",
      "fold 2\n",
      "Calibration error = 0.92\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`bins` must be positive, when an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 159\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#SHARPNESS plots and value\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m#should a be sharp (i.e. uncertainties should be as small as possible.)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m#Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m stdevs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_ensemble_uncertanties)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m#right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m sharpness, dispersion \u001b[38;5;241m=\u001b[39m \u001b[43mplot_sharpness_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstdevs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_saving_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m#DISPERSION value\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m#meaningful UQ, having a large dispersion of uncertainties is valuable. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#                                                         show_figure=False,\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#                                                         well_sampled_number=0.025)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''using their library to make an rve plot'''\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models/Paper2/Bagging_models.py:504\u001b[0m, in \u001b[0;36mplot_sharpness_curve\u001b[0;34m(stdevs, saving_folder)\u001b[0m\n\u001b[1;32m    502\u001b[0m fig_sharp \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# ax_sharp = sns.histplot(stdevs, kde=False, norm_hist=True)\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m ax_sharp \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstdevs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdensity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m ax_sharp\u001b[38;5;241m.\u001b[39mset_xlim(xlim)\n\u001b[1;32m    506\u001b[0m ax_sharp\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted standard deviation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/seaborn/distributions.py:1432\u001b[0m, in \u001b[0;36mhistplot\u001b[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m estimate_kws \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1422\u001b[0m     stat\u001b[38;5;241m=\u001b[39mstat,\n\u001b[1;32m   1423\u001b[0m     bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     cumulative\u001b[38;5;241m=\u001b[39mcumulative,\n\u001b[1;32m   1428\u001b[0m )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39munivariate:\n\u001b[0;32m-> 1432\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_univariate_histogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommon_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommon_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommon_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkde_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkde_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimate_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimate_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mline_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1450\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_bivariate_histogram(\n\u001b[1;32m   1451\u001b[0m         common_bins\u001b[38;5;241m=\u001b[39mcommon_bins,\n\u001b[1;32m   1452\u001b[0m         common_norm\u001b[38;5;241m=\u001b[39mcommon_norm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1463\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/seaborn/distributions.py:476\u001b[0m, in \u001b[0;36m_DistributionPlotter.plot_univariate_histogram\u001b[0;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (multiple_histograms \u001b[38;5;129;01mand\u001b[39;00m common_bins):\n\u001b[1;32m    475\u001b[0m     bin_kws \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m_define_bin_params(sub_data, orient, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 476\u001b[0m res \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m_normalize(\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_kws\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    477\u001b[0m heights \u001b[38;5;241m=\u001b[39m res[estimator\u001b[38;5;241m.\u001b[39mstat]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    478\u001b[0m widths \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/seaborn/_stats/counting.py:176\u001b[0m, in \u001b[0;36mHist._eval\u001b[0;34m(self, data, orient, bin_kws)\u001b[0m\n\u001b[1;32m    173\u001b[0m weights \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 176\u001b[0m hist, edges \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbin_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m width \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiff(edges)\n\u001b[1;32m    179\u001b[0m center \u001b[38;5;241m=\u001b[39m edges[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m width \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/numpy/lib/histograms.py:793\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 793\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/numpy/lib/histograms.py:424\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be an integer, a string, or an array\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m _get_outer_edges(a, \u001b[38;5;28mrange\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: `bins` must be positive, when an integer"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Helper function predicts the labels of the featureset given, and the uncertainty.\n",
    "Also compares this to the true labels of the dataset.\n",
    "'''\n",
    "def Get_predictions_and_uncertainty(test_features_path, test_labels_path, model_folder, saving_folder, features_to_keep, label_to_predict, model_type):\n",
    "    if(not os.path.exists(saving_folder)): os.makedirs(saving_folder)\n",
    "    test_features = pd.read_csv(test_features_path)[features_to_keep]\n",
    "    test_labels = pd.read_csv(test_labels_path) \n",
    "\n",
    "    filename = model_folder + '/model_no1.sav'\n",
    "    with open(os.path.join(model_folder, filename), 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    # Loop through the pickle files in the folder\n",
    "    # for filename in os.listdir(model_folder):\n",
    "    #     if filename.endswith('.sav'):\n",
    "    #         # Load the model from the pickle file\n",
    "    #         with open(os.path.join(model_folder, filename), 'rb') as file:\n",
    "    #             model = pickle.load(file)\n",
    "    #             models.append(model)\n",
    "    \n",
    "    all_model_predictions = []\n",
    "    # for model in models:\n",
    "    if(model_type == 'Single RF'): \n",
    "        \n",
    "        tree_predictions = []\n",
    "        # Iterate over all trees in the random forest\n",
    "        for tree in model.estimators_:\n",
    "            # Predict using the current tree\n",
    "            tree_pred = tree.predict(test_features.to_numpy())\n",
    "            # Append the predictions to the list\n",
    "            tree_predictions.append(tree_pred)\n",
    "\n",
    "        # Convert the list to a NumPy array for easier manipulation if needed\n",
    "        tree_predictions = np.array(tree_predictions)\n",
    "        \n",
    "        # current_predictions = model.predict(test_features.to_numpy()) #COMMENT this is the same as the average of all the individual trees\n",
    "        current_predictions = np.mean(tree_predictions, axis=0)\n",
    "        single_pred_stds = np.std(tree_predictions, axis=0)\n",
    "        \n",
    "    elif(model_type == 'Single GPR'):\n",
    "        current_predictions, single_pred_stds = model.predict(test_features.to_numpy(), return_std=True)\n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "            \n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "        current_predictions = current_predictions.reshape(current_predictions.shape[0])\n",
    "        all_model_predictions.append(current_predictions)\n",
    "    \n",
    "    elif(model_type == 'NN_fed_GPR'):\n",
    "        current_predictions, single_pred_stds = model.predict(test_features.to_numpy())\n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "            \n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "        current_predictions = current_predictions.reshape(current_predictions.shape[0])\n",
    "        all_model_predictions.append(current_predictions)\n",
    "        \n",
    "    elif(model_type == 'NN_fed_RF'):\n",
    "        current_predictions, single_pred_stds = model.predict(test_features.to_numpy())\n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "            \n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "        current_predictions = current_predictions.reshape(current_predictions.shape[0])\n",
    "        all_model_predictions.append(current_predictions)\n",
    "    \n",
    "    elif(model_type == 'RF_fed_GPR'):\n",
    "        current_predictions, single_pred_stds = model.predict(test_features.to_numpy())\n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "            \n",
    "        # current_predictions = model.predict(test_features.to_numpy())\n",
    "        current_predictions = current_predictions.reshape(current_predictions.shape[0])\n",
    "        all_model_predictions.append(current_predictions)\n",
    "\n",
    "    all_model_predictions = np.array(all_model_predictions)\n",
    "    \n",
    "    # ensemble_predictions = []\n",
    "    # ensemble_uncertanties = []\n",
    "    # for label_no in range(len(test_labels)):\n",
    "    #     true_label = test_labels.iloc[label_no][0]\n",
    "    #     mean_prediction, std_prediction = np.mean(all_model_predictions[:, label_no]), np.std(all_model_predictions[:, label_no])\n",
    "    #     ensemble_predictions.append(mean_prediction)\n",
    "    #     ensemble_uncertanties.append(std_prediction*2) #uncertainty will be 2 * the std of the predictions\n",
    "    \n",
    "    uncertainties = single_pred_stds * 2\n",
    "    test_or_train = test_features_path.split('_')[-2].split('/')[-1]\n",
    "    r2 = parody_plot_with_std(test_labels.to_numpy(), current_predictions, uncertainties, saving_folder, label_to_predict, model_type, testtrain=test_or_train)\n",
    "    \n",
    "    return r2, current_predictions, uncertainties, test_labels\n",
    "\n",
    "\n",
    "'''Now we will evaluate the performance of the bagging models'''\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'MODEL TYPE = {model_type}')\n",
    "    for label_to_predict in labels_to_predict:\n",
    "        print(f'LABEL = {label_to_predict}')\n",
    "        performance_data = []\n",
    "        for fold_no in range(1,6):\n",
    "            print(f'fold {fold_no}')\n",
    "            \n",
    "            #defining folders to get the models and to store the results\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            \n",
    "            #defining folders where the datasets are coming from (5-fold cv)\n",
    "            test_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_features.csv'\n",
    "            test_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_labels.csv'\n",
    "            train_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_features.csv'\n",
    "            train_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_labels.csv'\n",
    "\n",
    "            #defining the features that each model used (since they vary with each model)\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            \n",
    "            #predicting the test and train sets with the bagging models\n",
    "            test_r2, test_ensemble_predictions, test_ensemble_uncertanties, test_labels = Get_predictions_and_uncertainty(test_features_path, test_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "            train_r2, train_ensemble_predictions, train_ensemble_uncertanties, train_labels = Get_predictions_and_uncertainty(train_features_path, train_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "\n",
    "            #defining the residual errors of the predictions\n",
    "            train_labels_arr = train_labels.to_numpy().T[0]\n",
    "            train_predictions_arr = np.array(train_ensemble_predictions)\n",
    "            test_labels_arr = test_labels.to_numpy().T[0]\n",
    "            test_predictions_arr = np.array(test_ensemble_predictions)\n",
    "            train_residuals = pd.Series(np.abs(train_labels_arr - train_predictions_arr))\n",
    "            test_residuals = pd.Series(np.abs(test_labels_arr - test_predictions_arr))\n",
    "\n",
    "            a = 0\n",
    "            b = 0\n",
    "            '''getting calibration factors *** linear'''\n",
    "            # cf = CorrectionFactors(train_residuals, pd.Series(train_ensemble_uncertanties))\n",
    "            # a, b = cf.nll()\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * np.array(train_ensemble_uncertanties) + b, name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * np.array(test_ensemble_uncertanties) + b, name='test_model_errors')\n",
    "            \n",
    "            '''getting calibration factors *** Nonlinear'''\n",
    "            # a, b = get_calibration_factors(train_residuals, train_ensemble_uncertanties)\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * (train_ensemble_uncertanties**((b/2) + 1)), name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * (test_ensemble_uncertanties**((b/2) + 1)), name='test_model_errors')\n",
    "\n",
    "            '''\n",
    "            Calculating and plotting performance metrics as outlined in section 2.3 of Tran et al. (https://dx.doi.org/10.1088/2632-2153/ab7e1a)\n",
    "            \n",
    "            Models should be compared in terms of \n",
    "            1st - accuracy (R^2) \n",
    "            2nd - calibration (miscalibration area)\n",
    "            3rd - sharpness\n",
    "            4th - dispersion\n",
    "            '''\n",
    "            #CALIBRATION plots and miscalibration area\n",
    "            #This tells us how 'honest' our uncertainty values are. A perfect calibration plot would mean for a given confidence interval in our prediction\n",
    "            #(say 90%), we can expect with 90% certainty that the true value falls within that confidence interval.\n",
    "            miscalibration_area, calibration_error = make_calibration_plots(model_type, test_predictions_arr, test_labels_arr, test_ensemble_uncertanties, results_saving_folder)\n",
    "            #SHARPNESS plots and value\n",
    "            #Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\n",
    "            #should a be sharp (i.e. uncertainties should be as small as possible.)\n",
    "            #Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\n",
    "            stdevs = np.array(test_ensemble_uncertanties)/2 #right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\n",
    "            sharpness, dispersion = plot_sharpness_curve(stdevs, results_saving_folder)\n",
    "            #DISPERSION value\n",
    "            #Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \n",
    "            #meaningful UQ, having a large dispersion of uncertainties is valuable. \n",
    "            #Dispersion is calculated using equation 4 of the paper, which is called the coefficient of variation (Cv). #COMMENT High dispersion (Cv) values are better.\n",
    "                \n",
    "            \n",
    "            # blank_model_for_plot = SklearnModel('RandomForestRegressor')\n",
    "            # mastml_RVE = Error()\n",
    "\n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='train', \n",
    "            #                                                         model_errors=pd.Series(train_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_train_uncertainties,\n",
    "            #                                                         residuals= train_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            \n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='test', \n",
    "            #                                                         model_errors=pd.Series(test_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_test_uncertainties,\n",
    "            #                                                         residuals= test_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            '''using their library to make an rve plot'''\n",
    "            # train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope = make_RVE_plots(label_to_predict, model_type, test_ensemble_predictions, test_ensemble_uncertanties, test_labels, train_ensemble_predictions, train_ensemble_uncertanties, train_labels, results_saving_folder, num_bins=15)\n",
    "            \n",
    "            '''collecting the performance data from this model'''\n",
    "            # performance_data.append([15, fold_no, train_r2, test_r2, a, b, train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope, miscalibration_area, calibration_error])\n",
    "            performance_data.append([fold_no, train_r2, test_r2, miscalibration_area, calibration_error, sharpness, dispersion])\n",
    "            \n",
    "        # columns = ['num bins', 'fold_no', 'train R2', 'test R2',  'a', 'b', 'train_intercept', 'train_slope', 'CAL_train_intercept', 'CAL_train_slope', 'train_intercept', 'test_slope', 'CAL_test_intercept', 'CAL_test_slope', 'miscal_area', 'cal_error']\n",
    "        columns = ['fold_no', 'train R2', 'test R2', 'miscal_area', 'cal_error', 'sharpness', 'dispersion']\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for row in performance_data:\n",
    "            df.loc[len(df)] = row\n",
    "        average_row = df.mean()\n",
    "        df = df.append(average_row, ignore_index=True)\n",
    "            \n",
    "        results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models'\n",
    "        df.to_csv(results_saving_folder + f'/{label_to_predict}_{model_type}_1results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
