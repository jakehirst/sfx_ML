{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''First, we need to define the path of where to get the dataset, and define other parameters that we will need'''\n",
    "import sys\n",
    "sys.path.append('/Users/jakehirst/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models')\n",
    "\n",
    "from NN_fed_GPR import *\n",
    "from NN_fed_RF import *\n",
    "from RF_fed_GPR import *\n",
    "from Bagging_models import *\n",
    "from Backward_feature_selection import *\n",
    "from Single_UQ_models import *\n",
    "import ast\n",
    "\n",
    "model_types = ['Single RF', 'Single GPR', 'NN_fed_GPR', 'NN_fed_RF', 'RF_fed_GPR']\n",
    "# model_types = ['NN_fed_RF', 'RF_fed_GPR']\n",
    "# model_types = [ 'RF_fed_GPR']\n",
    "\n",
    "# model_types = ['Single RF']\n",
    "model_types = ['Single GPR']\n",
    "\n",
    "all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "\n",
    "labels_to_predict = ['impact site x', 'impact site y', 'height']\n",
    "# labels_to_predict = ['height']\n",
    "\n",
    "with_or_without_transformations = 'with'\n",
    "with_or_without_transformations = 'without'\n",
    "\n",
    "# Paper2_path = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations_trial_2'\n",
    "Paper2_path = f'/Volumes/Jake_ssd/with_timestep_init'\n",
    "model_folder = Paper2_path + f'/UQ_bagging_models_{with_or_without_transformations}_transformations'\n",
    "data_folder = Paper2_path + '/5fold_datasets'\n",
    "results_folder = Paper2_path + '/Compare_Code_5_fold_ensemble_results'\n",
    "# hyperparam_folder = Paper2_path + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "hyperparam_folder = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations' + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "\n",
    "image_folder = '/Users/jakehirst/Desktop/sfx/sfx_ML_data/images_sfx/new_dataset/Visible_cracks'\n",
    "\n",
    "if(with_or_without_transformations == 'with'):\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/dataset/feature_transformations_2023-11-16/height/HEIGHTALL_TRANSFORMED_FEATURES.csv\"\n",
    "    backward_feat_selection_results_folder = '/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/results'\n",
    "else:\n",
    "    # full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_no_feature_engineering/dataset/New_Crack_Len_FULL_OG_dataframe_2023_11_16.csv\"\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 2/New_Crack_Len_FULL_OG_dataframe_2024_02_22.csv\"\n",
    "    backward_feat_selection_results_folder = Paper2_path + '/Paper_2_results_WITHOUT_feature_engineering/results' \n",
    "    df = pd.read_csv(full_dataset_pathname, index_col=0)\n",
    "    all_features = df.columns\n",
    "    all_features = all_features.drop(all_labels)\n",
    "    all_features = str(all_features.drop('timestep_init').to_list()) #TODO CHANGE BELOW LINE BACK TO THIS ONCE DONE DOING THE TIMESTEP INIT ANALYSIS\n",
    "    # all_features = str(all_features.to_list())\n",
    "    print(all_features)\n",
    "    \n",
    "    \n",
    "'''Only have to uncomment this if the 5 fold datasets have not been made or need to be remade'''\n",
    "# make_5_fold_datasets(data_folder, full_dataset_pathname, all_labels=all_labels)\n",
    "\n",
    "print('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using just the basic features\n",
      "using just the basic features\n",
      "using just the basic features\n",
      "{'impact site x': {'Single GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}, 'impact site y': {'Single GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}, 'height': {'Single GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}}\n"
     ]
    }
   ],
   "source": [
    "'''get the appropriate features that each model will use based on backward feature elimination'''\n",
    "all_features_to_keep = {}\n",
    "\n",
    "min_features = 1 #minimum number of features you want to select from BFS (backward feature selection)\n",
    "max_features = 25 #maximum number of features you want to select from BFS\n",
    "for label in labels_to_predict:\n",
    "    all_features_to_keep[label] = {}\n",
    "    for model_type in model_types:\n",
    "        \n",
    "        if('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname):\n",
    "            print('true')\n",
    "            model_type_hyperparam = model_type.removeprefix('Single ')\n",
    "            #TODO use code below if using feature selection\n",
    "            best_features = get_best_features(backward_feat_selection_results_folder, label, model_type_hyperparam, min_features, max_features)\n",
    "            all_features_to_keep[label][model_type] = best_features\n",
    "        \n",
    "        else:\n",
    "            print('using just the basic features')\n",
    "            #TODO use code below if NOT using feature selection\n",
    "            all_features_to_keep[label][model_type] = all_features\n",
    "\n",
    "print(all_features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site x using Single GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/with_timestep_init/5fold_datasets/impact site x/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakehirst/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__constant_value is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jakehirst/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified lower bound 100.0. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using Single GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/with_timestep_init/5fold_datasets/impact site y/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m features_to_keep \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(all_features_to_keep[label_to_predict][model_type])\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_to_keep)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mmake_UQ_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_saving_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_to_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparam_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mmake_UQ_model\u001b[0;34m(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, num_training_points, model_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# model = GaussianProcessRegressor(kernel=kernel, random_state=0, n_restarts_optimizer=200)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     model \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(kernel\u001b[38;5;241m=\u001b[39mkernel, n_restarts_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m) \u001b[38;5;66;03m#COMMENT removed random state\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     save_ensemble_model(model, \u001b[38;5;241m1\u001b[39m, model_saving_folder) \n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN_fed_GPR\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:321\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_restarts_optimizer):\n\u001b[1;32m    319\u001b[0m         theta_initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39muniform(bounds[:, \u001b[38;5;241m0\u001b[39m], bounds[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    320\u001b[0m         optima\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 321\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constrained_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m         )\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Select result from run with minimal (negative) log-marginal\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# likelihood\u001b[39;00m\n\u001b[1;32m    325\u001b[0m lml_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(itemgetter(\u001b[38;5;241m1\u001b[39m), optima))\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:652\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constrained_optimization\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin_l_bfgs_b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 652\u001b[0m         opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m         _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res)\n\u001b[1;32m    660\u001b[0m         theta_opt, func_min \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/optimize/_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:293\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[0;32m--> 293\u001b[0m         lml, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_marginal_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlml, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:583\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    581\u001b[0m K[np\u001b[38;5;241m.\u001b[39mdiag_indices_from(K)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPR_CHOLESKY_LOWER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39mzeros_like(theta)) \u001b[38;5;28;01mif\u001b[39;00m eval_gradient \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py:89\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(a, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     c, lower \u001b[38;5;241m=\u001b[39m \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/miniconda3/envs/tfp_env/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py:35\u001b[0m, in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     33\u001b[0m overwrite_a \u001b[38;5;241m=\u001b[39m overwrite_a \u001b[38;5;129;01mor\u001b[39;00m _datacopied(a1, a)\n\u001b[1;32m     34\u001b[0m potrf, \u001b[38;5;241m=\u001b[39m get_lapack_funcs((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotrf\u001b[39m\u001b[38;5;124m'\u001b[39m,), (a1,))\n\u001b[0;32m---> 35\u001b[0m c, info \u001b[38;5;241m=\u001b[39m \u001b[43mpotrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-th leading minor of the array is not positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m info)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Now we will make all of the models'''\n",
    "\n",
    "\n",
    "def make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, num_training_points=False, model_type=None): \n",
    "    models = []\n",
    "    training_features = training_features[features_to_keep]\n",
    "    current_label = training_labels.columns[0]\n",
    "    if(not os.path.exists(model_saving_folder)): os.mkdir(model_saving_folder)\n",
    "\n",
    "    if(model_type == 'Single RF'):\n",
    "        hp_folder = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations_trial_2/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "        depth, features, samples_leaf, samples_split, estimators = get_best_hyperparameters_RF(label_to_predict=training_labels.columns[0], hyperparameter_folder=hp_folder)\n",
    "        model =  RandomForestRegressor(max_depth=depth, max_features=features, \n",
    "                                       min_samples_leaf = samples_leaf, min_samples_split = samples_split, n_estimators=estimators, random_state=42)\n",
    "        model.fit(training_features, training_labels)\n",
    "        \n",
    "    elif(model_type == 'Single GPR'):\n",
    "\n",
    "        kernel = ConstantKernel(constant_value_bounds=(1e-3, 1e3)) * RBF(length_scale_bounds=(1e2, 1e6)) + WhiteKernel(noise_level_bounds=(1e-10, 1e+3)) \n",
    "        # model = GaussianProcessRegressor(kernel=kernel, random_state=0, n_restarts_optimizer=200)\n",
    "        model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=200) #COMMENT removed random state\n",
    "\n",
    "        model.fit(training_features.to_numpy(), training_labels.to_numpy())\n",
    "        save_ensemble_model(model, 1, model_saving_folder) \n",
    "\n",
    "    elif(model_type == 'NN_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "\n",
    "    elif(model_type == 'RF_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = RF_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "        \n",
    "    elif(model_type == 'NN_fed_RF'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_RF()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder, num_optimization_tries=100, hyperparam_folder=f'/Volumes/Jake_ssd/Paper 2/without_transformations/optimized_hyperparams/NN_fed_RF/{current_label}')\n",
    "        save_ensemble_model(model, 1, model_saving_folder) \n",
    "\n",
    "    save_ensemble_model(model, 1, model_saving_folder) \n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "for fold_no in range(1,6):\n",
    "    for model_type in model_types:\n",
    "        for label_to_predict in labels_to_predict:\n",
    "            print(f'\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting {label_to_predict} using {model_type} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n')\n",
    "            \n",
    "            # all_labels = ['height', 'phi', 'theta', \n",
    "            #             'impact site x', 'impact site y', 'impact site z', \n",
    "            #             'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "            print(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv')\n",
    "            training_features = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv').reset_index(drop=True)\n",
    "            training_labels = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_labels.csv').reset_index(drop=True)\n",
    "\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(model_saving_folder)):\n",
    "                os.makedirs(model_saving_folder)\n",
    "                \n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(results_saving_folder)):\n",
    "                os.makedirs(results_saving_folder)\n",
    "            # make_dirs(model_saving_folder)\n",
    "            # make_dirs(results_saving_folder)\n",
    "\n",
    "            '''TODO gotta find out what features to use for each label before testing on new dataset'''\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            print(features_to_keep)\n",
    "            make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, 1, features_to_keep, hyperparam_folder, model_type=model_type)\n",
    "            # make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''Now we will evaluate the performance of the bagging models'''\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'MODEL TYPE = {model_type}')\n",
    "    for label_to_predict in labels_to_predict:\n",
    "        print(f'LABEL = {label_to_predict}')\n",
    "        performance_data = []\n",
    "        for fold_no in range(1,6):\n",
    "            print(f'fold {fold_no}')\n",
    "            \n",
    "            #defining folders to get the models and to store the results\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            \n",
    "            #defining folders where the datasets are coming from (5-fold cv)\n",
    "            test_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_features.csv'\n",
    "            test_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_labels.csv'\n",
    "            train_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_features.csv'\n",
    "            train_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_labels.csv'\n",
    "\n",
    "            #defining the features that each model used (since they vary with each model)\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            \n",
    "            #predicting the test and train sets with the bagging models\n",
    "            test_r2, test_ensemble_predictions, test_ensemble_uncertanties, test_labels = Get_predictions_and_uncertainty_single_model(test_features_path, test_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "            train_r2, train_ensemble_predictions, train_ensemble_uncertanties, train_labels = Get_predictions_and_uncertainty_single_model(train_features_path, train_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "\n",
    "            #defining the residual errors of the predictions\n",
    "            train_labels_arr = train_labels\n",
    "            train_predictions_arr = np.array(train_ensemble_predictions)\n",
    "            test_labels_arr = test_labels\n",
    "            test_predictions_arr = np.array(test_ensemble_predictions)\n",
    "            train_residuals = pd.Series(np.abs(train_labels_arr - train_predictions_arr))\n",
    "            test_residuals = pd.Series(np.abs(test_labels_arr - test_predictions_arr))\n",
    "\n",
    "            a = 0\n",
    "            b = 0\n",
    "            '''getting calibration factors *** linear'''\n",
    "            # cf = CorrectionFactors(train_residuals, pd.Series(train_ensemble_uncertanties))\n",
    "            # a, b = cf.nll()\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * np.array(train_ensemble_uncertanties) + b, name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * np.array(test_ensemble_uncertanties) + b, name='test_model_errors')\n",
    "            \n",
    "            '''getting calibration factors *** Nonlinear'''\n",
    "            # a, b = get_calibration_factors(train_residuals, train_ensemble_uncertanties)\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * (train_ensemble_uncertanties**((b/2) + 1)), name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * (test_ensemble_uncertanties**((b/2) + 1)), name='test_model_errors')\n",
    "\n",
    "            '''\n",
    "            Calculating and plotting performance metrics as outlined in section 2.3 of Tran et al. (https://dx.doi.org/10.1088/2632-2153/ab7e1a)\n",
    "            \n",
    "            Models should be compared in terms of \n",
    "            1st - accuracy (R^2) \n",
    "            2nd - calibration (miscalibration area)\n",
    "            3rd - sharpness\n",
    "            4th - dispersion\n",
    "            '''\n",
    "            #CALIBRATION plots and miscalibration area\n",
    "            #This tells us how 'honest' our uncertainty values are. A perfect calibration plot would mean for a given confidence interval in our prediction\n",
    "            #(say 90%), we can expect with 90% certainty that the true value falls within that confidence interval.\n",
    "            miscalibration_area, calibration_error = make_calibration_plots(model_type, test_predictions_arr, test_labels_arr, test_ensemble_uncertanties, results_saving_folder)\n",
    "            #SHARPNESS plots and value\n",
    "            #Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\n",
    "            #should a be sharp (i.e. uncertainties should be as small as possible.)\n",
    "            #Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\n",
    "            stdevs = np.array(test_ensemble_uncertanties)/2 #right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\n",
    "            sharpness, dispersion = plot_sharpness_curve(stdevs, results_saving_folder)\n",
    "            #DISPERSION value\n",
    "            #Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \n",
    "            #meaningful UQ, having a large dispersion of uncertainties is valuable. \n",
    "            #Dispersion is calculated using equation 4 of the paper, which is called the coefficient of variation (Cv). #COMMENT High dispersion (Cv) values are better.\n",
    "                \n",
    "            \n",
    "            # blank_model_for_plot = SklearnModel('RandomForestRegressor')\n",
    "            # mastml_RVE = Error()\n",
    "\n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='train', \n",
    "            #                                                         model_errors=pd.Series(train_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_train_uncertainties,\n",
    "            #                                                         residuals= train_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            \n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='test', \n",
    "            #                                                         model_errors=pd.Series(test_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_test_uncertainties,\n",
    "            #                                                         residuals= test_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            '''using their library to make an rve plot'''\n",
    "            # train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope = make_RVE_plots(label_to_predict, model_type, test_ensemble_predictions, test_ensemble_uncertanties, test_labels, train_ensemble_predictions, train_ensemble_uncertanties, train_labels, results_saving_folder, num_bins=15)\n",
    "            \n",
    "            '''collecting the performance data from this model'''\n",
    "            # performance_data.append([15, fold_no, train_r2, test_r2, a, b, train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope, miscalibration_area, calibration_error])\n",
    "            performance_data.append([fold_no, train_r2, test_r2, miscalibration_area, calibration_error, sharpness, dispersion])\n",
    "            \n",
    "        # columns = ['num bins', 'fold_no', 'train R2', 'test R2',  'a', 'b', 'train_intercept', 'train_slope', 'CAL_train_intercept', 'CAL_train_slope', 'train_intercept', 'test_slope', 'CAL_test_intercept', 'CAL_test_slope', 'miscal_area', 'cal_error']\n",
    "        columns = ['fold_no', 'train R2', 'test R2', 'miscal_area', 'cal_error', 'sharpness', 'dispersion']\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for row in performance_data:\n",
    "            df.loc[len(df)] = row\n",
    "        average_row = df.mean()\n",
    "        df = df.append(average_row, ignore_index=True)\n",
    "            \n",
    "        results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models'\n",
    "        df.to_csv(results_saving_folder + f'/{label_to_predict}_{model_type}_1results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
