{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''First, we need to define the path of where to get the dataset, and define other parameters that we will need'''\n",
    "import sys\n",
    "sys.path.append('/Users/jakehirst/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models')\n",
    "\n",
    "from NN_fed_GPR import *\n",
    "from NN_fed_RF import *\n",
    "from RF_fed_GPR import *\n",
    "from Bagging_models import *\n",
    "from Backward_feature_selection import *\n",
    "from Single_UQ_models import *\n",
    "import ast\n",
    "\n",
    "model_types = ['Single RF', 'Single GPR', 'NN_fed_GPR', 'NN_fed_RF', 'RF_fed_GPR']\n",
    "# model_types = ['NN_fed_RF', 'RF_fed_GPR']\n",
    "# model_types = [ 'RF_fed_GPR']\n",
    "\n",
    "# model_types = ['Single RF']\n",
    "model_types = ['Single GPR']\n",
    "model_types = [ 'RF_fed_GPR']\n",
    "\n",
    "all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "\n",
    "labels_to_predict = ['impact site x', 'impact site y', 'height']\n",
    "labels_to_predict = ['height']\n",
    "\n",
    "with_or_without_transformations = 'with'\n",
    "with_or_without_transformations = 'without'\n",
    "\n",
    "# Paper2_path = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations_trial_2'\n",
    "Paper2_path = f'/Volumes/Jake_ssd/with_timestep_init'\n",
    "model_folder = Paper2_path + f'/UQ_bagging_models_{with_or_without_transformations}_transformations'\n",
    "data_folder = Paper2_path + '/5fold_datasets'\n",
    "results_folder = Paper2_path + '/Compare_Code_5_fold_ensemble_results'\n",
    "# hyperparam_folder = Paper2_path + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "hyperparam_folder = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations' + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "\n",
    "image_folder = '/Users/jakehirst/Desktop/sfx/sfx_ML_data/images_sfx/new_dataset/Visible_cracks'\n",
    "\n",
    "if(with_or_without_transformations == 'with'):\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/dataset/feature_transformations_2023-11-16/height/HEIGHTALL_TRANSFORMED_FEATURES.csv\"\n",
    "    backward_feat_selection_results_folder = '/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/results'\n",
    "else:\n",
    "    # full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_no_feature_engineering/dataset/New_Crack_Len_FULL_OG_dataframe_2023_11_16.csv\"\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 2/New_Crack_Len_FULL_OG_dataframe_2024_02_22.csv\"\n",
    "    backward_feat_selection_results_folder = Paper2_path + '/Paper_2_results_WITHOUT_feature_engineering/results' \n",
    "    df = pd.read_csv(full_dataset_pathname, index_col=0)\n",
    "    all_features = df.columns\n",
    "    all_features = all_features.drop(all_labels)\n",
    "    all_features = str(all_features.drop('timestep_init').to_list()) #TODO CHANGE BELOW LINE BACK TO THIS ONCE DONE DOING THE TIMESTEP INIT ANALYSIS\n",
    "    # all_features = str(all_features.to_list())\n",
    "    print(all_features)\n",
    "    \n",
    "    \n",
    "'''Only have to uncomment this if the 5 fold datasets have not been made or need to be remade'''\n",
    "# make_5_fold_datasets(data_folder, full_dataset_pathname, all_labels=all_labels)\n",
    "\n",
    "print('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using just the basic features\n",
      "{'height': {'RF_fed_GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}}\n"
     ]
    }
   ],
   "source": [
    "'''get the appropriate features that each model will use based on backward feature elimination'''\n",
    "all_features_to_keep = {}\n",
    "\n",
    "min_features = 1 #minimum number of features you want to select from BFS (backward feature selection)\n",
    "max_features = 25 #maximum number of features you want to select from BFS\n",
    "for label in labels_to_predict:\n",
    "    all_features_to_keep[label] = {}\n",
    "    for model_type in model_types:\n",
    "        \n",
    "        if('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname):\n",
    "            print('true')\n",
    "            model_type_hyperparam = model_type.removeprefix('Single ')\n",
    "            #TODO use code below if using feature selection\n",
    "            best_features = get_best_features(backward_feat_selection_results_folder, label, model_type_hyperparam, min_features, max_features)\n",
    "            all_features_to_keep[label][model_type] = best_features\n",
    "        \n",
    "        else:\n",
    "            print('using just the basic features')\n",
    "            #TODO use code below if NOT using feature selection\n",
    "            all_features_to_keep[label][model_type] = all_features\n",
    "\n",
    "print(all_features_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using RF_fed_GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/with_timestep_init/5fold_datasets/height/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakehirst/miniconda3/envs/tfp_env/lib/python3.10/site-packages/sklearn/base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''Now we will make all of the models'''\n",
    "\n",
    "\n",
    "def make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, num_training_points=False, model_type=None): \n",
    "    models = []\n",
    "    training_features = training_features[features_to_keep]\n",
    "    current_label = training_labels.columns[0]\n",
    "    if(not os.path.exists(model_saving_folder)): os.mkdir(model_saving_folder)\n",
    "\n",
    "    if(model_type == 'Single RF'):\n",
    "        hp_folder = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations_trial_2/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "        depth, features, samples_leaf, samples_split, estimators = get_best_hyperparameters_RF(label_to_predict=training_labels.columns[0], hyperparameter_folder=hp_folder)\n",
    "        model =  RandomForestRegressor(max_depth=depth, max_features=features, \n",
    "                                       min_samples_leaf = samples_leaf, min_samples_split = samples_split, n_estimators=estimators, random_state=42)\n",
    "        model.fit(training_features, training_labels)\n",
    "        \n",
    "    elif(model_type == 'Single GPR'):\n",
    "\n",
    "        kernel = ConstantKernel(constant_value_bounds=(1e-3, 1e3)) * RBF(length_scale_bounds=(1e2, 1e6)) + WhiteKernel(noise_level_bounds=(1e-10, 1e+3)) \n",
    "        # model = GaussianProcessRegressor(kernel=kernel, random_state=0, n_restarts_optimizer=200)\n",
    "        model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=200) #COMMENT removed random state\n",
    "\n",
    "        model.fit(training_features.to_numpy(), training_labels.to_numpy())\n",
    "        save_ensemble_model(model, 1, model_saving_folder) \n",
    "\n",
    "    elif(model_type == 'NN_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "\n",
    "    elif(model_type == 'RF_fed_GPR'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = RF_fed_GPR()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder)\n",
    "        \n",
    "    elif(model_type == 'NN_fed_RF'):\n",
    "        # c, length_scale, noise_level = get_best_hyperparameters_NN_fed_GPR(label_to_predict=training_labels.columns[0], hyperparameter_folder=hyperparam_folder)\n",
    "        model = NN_fed_RF()\n",
    "        model.fit(training_features, training_labels, hyperparam_folder, num_optimization_tries=100, hyperparam_folder=f'/Volumes/Jake_ssd/Paper 2/without_transformations/optimized_hyperparams/NN_fed_RF/{current_label}')\n",
    "        save_ensemble_model(model, 1, model_saving_folder) \n",
    "\n",
    "    save_ensemble_model(model, 1, model_saving_folder) \n",
    "    # save_ensemble_model(model, 1, '/Users/jakehirst/Desktop/TEST FOLDER')\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "for fold_no in range(1,6):\n",
    "    for model_type in model_types:\n",
    "        for label_to_predict in labels_to_predict:\n",
    "            print(f'\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting {label_to_predict} using {model_type} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n')\n",
    "            \n",
    "            # all_labels = ['height', 'phi', 'theta', \n",
    "            #             'impact site x', 'impact site y', 'impact site z', \n",
    "            #             'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "            print(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv')\n",
    "            training_features = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv').reset_index(drop=True)\n",
    "            training_labels = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_labels.csv').reset_index(drop=True)\n",
    "\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(model_saving_folder)):\n",
    "                os.makedirs(model_saving_folder)\n",
    "                \n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            if(not os.path.exists(results_saving_folder)):\n",
    "                os.makedirs(results_saving_folder)\n",
    "            # make_dirs(model_saving_folder)\n",
    "            # make_dirs(results_saving_folder)\n",
    "\n",
    "            '''TODO gotta find out what features to use for each label before testing on new dataset'''\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            print(features_to_keep)\n",
    "            make_UQ_model(training_features, training_labels, model_saving_folder, label_to_predict, 1, features_to_keep, hyperparam_folder, model_type=model_type)\n",
    "            # make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''Now we will evaluate the performance of the bagging models'''\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'MODEL TYPE = {model_type}')\n",
    "    for label_to_predict in labels_to_predict:\n",
    "        print(f'LABEL = {label_to_predict}')\n",
    "        performance_data = []\n",
    "        for fold_no in range(1,6):\n",
    "            print(f'fold {fold_no}')\n",
    "            \n",
    "            #defining folders to get the models and to store the results\n",
    "            model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models/fold_{fold_no}'\n",
    "            \n",
    "            #defining folders where the datasets are coming from (5-fold cv)\n",
    "            test_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_features.csv'\n",
    "            test_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_labels.csv'\n",
    "            train_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_features.csv'\n",
    "            train_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_labels.csv'\n",
    "\n",
    "            #defining the features that each model used (since they vary with each model)\n",
    "            features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "            \n",
    "            #predicting the test and train sets with the bagging models\n",
    "            test_r2, test_ensemble_predictions, test_ensemble_uncertanties, test_labels = Get_predictions_and_uncertainty_single_model(test_features_path, test_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "            train_r2, train_ensemble_predictions, train_ensemble_uncertanties, train_labels = Get_predictions_and_uncertainty_single_model(train_features_path, train_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "\n",
    "            #defining the residual errors of the predictions\n",
    "            train_labels_arr = train_labels\n",
    "            train_predictions_arr = np.array(train_ensemble_predictions)\n",
    "            test_labels_arr = test_labels\n",
    "            test_predictions_arr = np.array(test_ensemble_predictions)\n",
    "            train_residuals = pd.Series(np.abs(train_labels_arr - train_predictions_arr))\n",
    "            test_residuals = pd.Series(np.abs(test_labels_arr - test_predictions_arr))\n",
    "\n",
    "            a = 0\n",
    "            b = 0\n",
    "            '''getting calibration factors *** linear'''\n",
    "            # cf = CorrectionFactors(train_residuals, pd.Series(train_ensemble_uncertanties))\n",
    "            # a, b = cf.nll()\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * np.array(train_ensemble_uncertanties) + b, name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * np.array(test_ensemble_uncertanties) + b, name='test_model_errors')\n",
    "            \n",
    "            '''getting calibration factors *** Nonlinear'''\n",
    "            # a, b = get_calibration_factors(train_residuals, train_ensemble_uncertanties)\n",
    "            # print(f'a = {a} b = {b}')\n",
    "            # calibrated_train_uncertainties = pd.Series(a * (train_ensemble_uncertanties**((b/2) + 1)), name='train_model_errors')\n",
    "            # calibrated_test_uncertainties = pd.Series(a * (test_ensemble_uncertanties**((b/2) + 1)), name='test_model_errors')\n",
    "\n",
    "            '''\n",
    "            Calculating and plotting performance metrics as outlined in section 2.3 of Tran et al. (https://dx.doi.org/10.1088/2632-2153/ab7e1a)\n",
    "            \n",
    "            Models should be compared in terms of \n",
    "            1st - accuracy (R^2) \n",
    "            2nd - calibration (miscalibration area)\n",
    "            3rd - sharpness\n",
    "            4th - dispersion\n",
    "            '''\n",
    "            #CALIBRATION plots and miscalibration area\n",
    "            #This tells us how 'honest' our uncertainty values are. A perfect calibration plot would mean for a given confidence interval in our prediction\n",
    "            #(say 90%), we can expect with 90% certainty that the true value falls within that confidence interval.\n",
    "            miscalibration_area, calibration_error = make_calibration_plots(model_type, test_predictions_arr, test_labels_arr, test_ensemble_uncertanties, results_saving_folder)\n",
    "            #SHARPNESS plots and value\n",
    "            #Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\n",
    "            #should a be sharp (i.e. uncertainties should be as small as possible.)\n",
    "            #Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\n",
    "            stdevs = np.array(test_ensemble_uncertanties)/2 #right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\n",
    "            sharpness, dispersion = plot_sharpness_curve(stdevs, results_saving_folder)\n",
    "            #DISPERSION value\n",
    "            #Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \n",
    "            #meaningful UQ, having a large dispersion of uncertainties is valuable. \n",
    "            #Dispersion is calculated using equation 4 of the paper, which is called the coefficient of variation (Cv). #COMMENT High dispersion (Cv) values are better.\n",
    "                \n",
    "            \n",
    "            # blank_model_for_plot = SklearnModel('RandomForestRegressor')\n",
    "            # mastml_RVE = Error()\n",
    "\n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='train', \n",
    "            #                                                         model_errors=pd.Series(train_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_train_uncertainties,\n",
    "            #                                                         residuals= train_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            \n",
    "            # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "            #                                                         model=blank_model_for_plot, \n",
    "            #                                                         data_type='test', \n",
    "            #                                                         model_errors=pd.Series(test_ensemble_uncertanties) ,\n",
    "            #                                                         model_errors_cal= calibrated_test_uncertainties,\n",
    "            #                                                         residuals= test_residuals, \n",
    "            #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "            #                                                         show_figure=False,\n",
    "            #                                                         well_sampled_number=0.025)\n",
    "            \n",
    "            '''using their library to make an rve plot'''\n",
    "            # train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope = make_RVE_plots(label_to_predict, model_type, test_ensemble_predictions, test_ensemble_uncertanties, test_labels, train_ensemble_predictions, train_ensemble_uncertanties, train_labels, results_saving_folder, num_bins=15)\n",
    "            \n",
    "            '''collecting the performance data from this model'''\n",
    "            # performance_data.append([15, fold_no, train_r2, test_r2, a, b, train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope, miscalibration_area, calibration_error])\n",
    "            performance_data.append([fold_no, train_r2, test_r2, miscalibration_area, calibration_error, sharpness, dispersion])\n",
    "            \n",
    "        # columns = ['num bins', 'fold_no', 'train R2', 'test R2',  'a', 'b', 'train_intercept', 'train_slope', 'CAL_train_intercept', 'CAL_train_slope', 'train_intercept', 'test_slope', 'CAL_test_intercept', 'CAL_test_slope', 'miscal_area', 'cal_error']\n",
    "        columns = ['fold_no', 'train R2', 'test R2', 'miscal_area', 'cal_error', 'sharpness', 'dispersion']\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        for row in performance_data:\n",
    "            df.loc[len(df)] = row\n",
    "        average_row = df.mean()\n",
    "        df = df.append(average_row, ignore_index=True)\n",
    "            \n",
    "        results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/1_models'\n",
    "        df.to_csv(results_saving_folder + f'/{label_to_predict}_{model_type}_1results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
