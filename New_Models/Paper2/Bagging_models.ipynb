{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the code in bagging_models.py for uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forestci is an optional dependency. To install latest forestci compatabilty with scikit-learn>=0.24, run pip install git+git://github.com/scikit-learn-contrib/forest-confidence-interval.git\n",
      "XGBoost is an optional dependency. If you want to use XGBoost models, please manually install xgboost package with pip install xgboost. If have error with finding libxgboost.dylib library, dobrew install libomp. If do not have brew on your system, first do ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" from the Terminal\n",
      "scikit-lego is an optional dependency, enabling use of the LowessRegression model. If you want to use this model, do \"pip install scikit-lego\"\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "'''First, we need to define the path of where to get the dataset, and define other parameters that we will need'''\n",
    "import sys\n",
    "sys.path.append('/Users/jakehirst/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models')\n",
    "\n",
    "from Bagging_models import *\n",
    "from Backward_feature_selection import *\n",
    "import ast\n",
    "\n",
    "model_types = ['ANN', 'RF', 'GPR','ridge']\n",
    "# model_types = ['GPR']\n",
    "all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "num_models_list = [20]\n",
    "labels_to_predict = ['impact site x', 'impact site y', 'height']\n",
    "# labels_to_predict = ['impact site x']\n",
    "\n",
    "with_or_without_transformations = 'with'\n",
    "# with_or_without_transformations = 'without'\n",
    "\n",
    "Paper2_path = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations'\n",
    "model_folder = Paper2_path + f'/UQ_bagging_models_{with_or_without_transformations}_transformations'\n",
    "data_folder = Paper2_path + '/5fold_datasets'\n",
    "results_folder = Paper2_path + '/Compare_Code_5_fold_ensemble_results'\n",
    "hyperparam_folder = Paper2_path + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "\n",
    "\n",
    "image_folder = '/Users/jakehirst/Desktop/sfx/sfx_ML_data/images_sfx/new_dataset/Visible_cracks'\n",
    "\n",
    "if(with_or_without_transformations == 'with'):\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/dataset/feature_transformations_2023-11-16/height/HEIGHTALL_TRANSFORMED_FEATURES.csv\"\n",
    "    backward_feat_selection_results_folder = '/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/results'\n",
    "else:\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_no_feature_engineering/dataset/New_Crack_Len_FULL_OG_dataframe_2023_11_16.csv\"\n",
    "    backward_feat_selection_results_folder = Paper2_path + '/Paper_2_results_WITHOUT_feature_engineering/results' \n",
    "    df = pd.read_csv(full_dataset_pathname, index_col=0)\n",
    "    all_features = df.columns\n",
    "    all_features = all_features.drop(all_labels)\n",
    "    all_features = str(all_features.drop('timestep_init').to_list())\n",
    "\n",
    "    print(all_features)\n",
    " \n",
    "\n",
    "\n",
    "'''Only have to uncomment this if the 5 fold datasets have not been made or need to be remade'''\n",
    "# make_5_fold_datasets(data_folder, full_dataset_pathname, image_folder)\n",
    "\n",
    "\n",
    "print('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "{'impact site x': {'ANN': \"['init x (unchanged)', 'linearity * std_thickness', 'init x + init y', 'init x + max_prop_speed', 'init x + std_kink']\", 'RF': \"['init x^2', 'init x^3', 'sqrt(init x)', 'exp(init x)', 'log(init x)', 'init x (unchanged)', 'init x * thickness_at_init', 'avg_prop_speed + init x', 'init x + max_kink', 'init x + thickness_at_init', 'init y + mean thickness', 'init y + thickness_at_init']\", 'GPR': \"['init x * init z', 'init z * thickness_at_init', 'linearity * max thickness', 'avg_ori + init x', 'crack len + init x', 'crack len + max thickness', 'crack len + std_thickness', 'dist btw frts + init x', 'dist btw frts + max thickness', 'dist btw frts + mean thickness', 'dist btw frts + std_thickness', 'init x + max thickness', 'init x + max_prop_speed', 'init x + std_kink', 'init x + std_thickness', 'max thickness + max_prop_speed']\", 'ridge': \"['init x^2', 'thickness_at_init^2', 'abs_val_mean_kink * thickness_at_init', 'crack len * thickness_at_init', 'dist btw frts * mean thickness', 'dist btw frts * median_thickness', 'init x * init z', 'init x * thickness_at_init', 'init z * max thickness', 'init z * thickness_at_init', 'linearity * max thickness', 'max thickness * sum_kink', 'max_prop_speed * median_thickness', 'mean thickness * sum_kink', 'median_kink * thickness_at_init', 'sum_kink * thickness_at_init', 'abs_val_sum_kink + init x', 'angle_btw + median_thickness', 'init x + init z', 'init z + thickness_at_init', 'sum_kink + thickness_at_init']\"}, 'impact site y': {'ANN': \"['sqrt(avg_prop_speed)', 'avg_ori * linearity', 'init y * init z', 'init y * mean_kink', 'linearity * thickness_at_init', 'max_kink / abs_val_sum_kink', 'abs_val_mean_kink / max_kink', 'abs_val_mean_kink + sum_kink', 'angle_btw + init y']\", 'RF': \"['init y^2', 'exp(init y)', 'init y (unchanged)', 'avg_prop_speed + init y']\", 'GPR': \"['sqrt(avg_prop_speed)', 'sqrt(dist btw frts)', 'avg_ori (unchanged)', 'init y (unchanged)', 'avg_ori * linearity', 'crack len * linearity', 'init y * mean_kink', 'crack len + init y', 'crack len + std_thickness']\", 'ridge': \"['sqrt(avg_prop_speed)', 'sqrt(thickness_at_init)', 'init y * mean_kink', 'init x + init y', 'init y + max_kink']\"}, 'height': {'ANN': \"['dist btw frts + init y', 'abs_val_sum_kink - linearity', 'linearity - abs_val_sum_kink', 'thickness_at_init - abs_val_sum_kink']\", 'RF': \"['abs_val_sum_kink * mean_kink', 'abs_val_sum_kink / init z', 'init z / abs_val_sum_kink', 'linearity / abs_val_sum_kink', 'abs_val_sum_kink - avg_prop_speed', 'linearity - abs_val_sum_kink']\", 'GPR': \"['abs_val_sum_kink / abs_val_mean_kink', 'abs_val_sum_kink / avg_prop_speed', 'abs_val_sum_kink + init y', 'dist btw frts + sum_kink', 'abs_val_sum_kink - linearity', 'linearity - abs_val_sum_kink']\", 'ridge': \"['init y * mean_kink', 'abs_val_sum_kink / avg_prop_speed', 'abs_val_sum_kink / mean_kink', 'abs_val_mean_kink ^ -max_prop_speed', 'abs_val_mean_kink + linearity', 'dist btw frts + median_kink']\"}}\n"
     ]
    }
   ],
   "source": [
    "'''get the appropriate features that each model will use based on backward feature elimination'''\n",
    "all_features_to_keep = {}\n",
    "\n",
    "min_features = 1 #minimum number of features you want to select from BFS (backward feature selection)\n",
    "max_features = 25 #maximum number of features you want to select from BFS\n",
    "for label in labels_to_predict:\n",
    "    all_features_to_keep[label] = {}\n",
    "    for model_type in model_types:\n",
    "        \n",
    "        if('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname):\n",
    "            print('true')\n",
    "        #TODO use code below if using feature selection\n",
    "            best_features = get_best_features(backward_feat_selection_results_folder, label, model_type, min_features, max_features)\n",
    "            all_features_to_keep[label][model_type] = best_features\n",
    "        \n",
    "        else:\n",
    "            print('using just the basic features')\n",
    "            #TODO use code below if NOT using feature selection\n",
    "            all_features_to_keep[label][model_type] = all_features\n",
    "\n",
    "print(all_features_to_keep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we will make all of the bagging models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Now we will make all of the bagging models'''\n",
    "# for fold_no in range(1,6):\n",
    "#     for model_type in model_types:\n",
    "#         for label_to_predict in labels_to_predict:\n",
    "#             for num_models in num_models_list:\n",
    "                \n",
    "#                 print(f'\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting {label_to_predict} using {model_type} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n')\n",
    "                \n",
    "#                 all_labels = ['height', 'phi', 'theta', \n",
    "#                             'impact site x', 'impact site y', 'impact site z', \n",
    "#                             'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "#                 print(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv')\n",
    "#                 training_features = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv').reset_index(drop=True)\n",
    "#                 training_labels = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_labels.csv').reset_index(drop=True)\n",
    "\n",
    "#                 model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "#                 if(not os.path.exists(model_saving_folder)):\n",
    "#                     os.makedirs(model_saving_folder)\n",
    "                    \n",
    "#                 results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "#                 if(not os.path.exists(results_saving_folder)):\n",
    "#                     os.makedirs(results_saving_folder)\n",
    "#                 # make_dirs(model_saving_folder)\n",
    "#                 # make_dirs(results_saving_folder)\n",
    "\n",
    "#                 '''TODO gotta find out what features to use for each label before testing on new dataset'''\n",
    "#                 features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "#                 print(features_to_keep)\n",
    "#                 make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE = ANN\n",
      "LABEL = impact site x\n",
      "fold 1\n",
      "Calibration error = 1.92\n",
      "fold 2\n",
      "Calibration error = 1.65\n",
      "fold 3\n",
      "Calibration error = 1.86\n",
      "fold 4\n",
      "Calibration error = 0.84\n",
      "fold 5\n",
      "Calibration error = 0.70\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 2.23\n",
      "fold 2\n",
      "Calibration error = 0.97\n",
      "fold 3\n",
      "Calibration error = 0.82\n",
      "fold 4\n",
      "Calibration error = 1.55\n",
      "fold 5\n",
      "Calibration error = 1.44\n",
      "LABEL = height\n",
      "fold 1\n",
      "Calibration error = 2.17\n",
      "fold 2\n",
      "Calibration error = 5.59\n",
      "fold 3\n",
      "Calibration error = 1.61\n",
      "fold 4\n",
      "Calibration error = 2.36\n",
      "fold 5\n",
      "Calibration error = 3.64\n",
      "MODEL TYPE = RF\n",
      "LABEL = impact site x\n",
      "fold 1\n",
      "Calibration error = 3.82\n",
      "fold 2\n",
      "Calibration error = 4.70\n",
      "fold 3\n",
      "Calibration error = 4.93\n",
      "fold 4\n",
      "Calibration error = 3.92\n",
      "fold 5\n",
      "Calibration error = 4.79\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 4.26\n",
      "fold 2\n",
      "Calibration error = 4.53\n",
      "fold 3\n",
      "Calibration error = 4.70\n",
      "fold 4\n",
      "Calibration error = 4.53\n",
      "fold 5\n",
      "Calibration error = 2.95\n",
      "LABEL = height\n",
      "fold 1\n",
      "Calibration error = 6.16\n",
      "fold 2\n",
      "Calibration error = 6.91\n",
      "fold 3\n",
      "Calibration error = 5.74\n",
      "fold 4\n",
      "Calibration error = 4.72\n",
      "fold 5\n",
      "Calibration error = 7.27\n",
      "MODEL TYPE = GPR\n",
      "LABEL = impact site x\n",
      "fold 1\n",
      "Calibration error = 7.67\n",
      "fold 2\n",
      "Calibration error = 5.10\n",
      "fold 3\n",
      "Calibration error = 5.46\n",
      "fold 4\n",
      "Calibration error = 8.06\n",
      "fold 5\n",
      "Calibration error = 4.55\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 4.59\n",
      "fold 2\n",
      "Calibration error = 4.37\n",
      "fold 3\n",
      "Calibration error = 5.97\n",
      "fold 4\n",
      "Calibration error = 4.92\n",
      "fold 5\n",
      "Calibration error = 4.98\n",
      "LABEL = height\n",
      "fold 1\n",
      "Calibration error = 6.62\n",
      "fold 2\n",
      "Calibration error = 6.72\n",
      "fold 3\n",
      "Calibration error = 6.18\n",
      "fold 4\n",
      "Calibration error = 6.39\n",
      "fold 5\n",
      "Calibration error = 7.80\n",
      "MODEL TYPE = ridge\n",
      "LABEL = impact site x\n",
      "fold 1\n",
      "Calibration error = 3.61\n",
      "fold 2\n",
      "Calibration error = 3.92\n",
      "fold 3\n",
      "Calibration error = 5.48\n",
      "fold 4\n",
      "Calibration error = 3.07\n",
      "fold 5\n",
      "Calibration error = 2.46\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 6.62\n",
      "fold 2\n",
      "Calibration error = 6.30\n",
      "fold 3\n",
      "Calibration error = 4.04\n",
      "fold 4\n",
      "Calibration error = 4.40\n",
      "fold 5\n",
      "Calibration error = 4.53\n",
      "LABEL = height\n",
      "fold 1\n",
      "Calibration error = 5.74\n",
      "fold 2\n",
      "Calibration error = 5.69\n",
      "fold 3\n",
      "Calibration error = 5.08\n",
      "fold 4\n",
      "Calibration error = 5.08\n",
      "fold 5\n",
      "Calibration error = 6.27\n"
     ]
    }
   ],
   "source": [
    "'''Now we will evaluate the performance of the bagging models'''\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'MODEL TYPE = {model_type}')\n",
    "    for label_to_predict in labels_to_predict:\n",
    "        print(f'LABEL = {label_to_predict}')\n",
    "        for num_models in num_models_list:\n",
    "            performance_data = []\n",
    "            for fold_no in range(1,6):\n",
    "                print(f'fold {fold_no}')\n",
    "\n",
    "                #defining folders to get the models and to store the results\n",
    "                model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                \n",
    "                #defining folders where the datasets are coming from (5-fold cv)\n",
    "                test_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_features.csv'\n",
    "                test_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_labels.csv'\n",
    "                train_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_features.csv'\n",
    "                train_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_labels.csv'\n",
    "\n",
    "                #defining the features that each model used (since they vary with each model)\n",
    "                features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "                \n",
    "                #predicting the test and train sets with the bagging models\n",
    "                test_r2, test_ensemble_predictions, test_ensemble_uncertanties, test_labels = Get_predictions_and_uncertainty_with_bagging(test_features_path, test_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "                train_r2, train_ensemble_predictions, train_ensemble_uncertanties, train_labels = Get_predictions_and_uncertainty_with_bagging(train_features_path, train_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "\n",
    "                #defining the residual errors of the predictions\n",
    "                train_labels_arr = train_labels.to_numpy().T[0]\n",
    "                train_predictions_arr = np.array(train_ensemble_predictions)\n",
    "                test_labels_arr = test_labels.to_numpy().T[0]\n",
    "                test_predictions_arr = np.array(test_ensemble_predictions)\n",
    "                train_residuals = pd.Series(np.abs(train_labels_arr - train_predictions_arr))\n",
    "                test_residuals = pd.Series(np.abs(test_labels_arr - test_predictions_arr))\n",
    "\n",
    "                a = 0\n",
    "                b = 0\n",
    "                '''getting calibration factors *** linear'''\n",
    "                # cf = CorrectionFactors(train_residuals, pd.Series(train_ensemble_uncertanties))\n",
    "                # a, b = cf.nll()\n",
    "                # print(f'a = {a} b = {b}')\n",
    "                # calibrated_train_uncertainties = pd.Series(a * np.array(train_ensemble_uncertanties) + b, name='train_model_errors')\n",
    "                # calibrated_test_uncertainties = pd.Series(a * np.array(test_ensemble_uncertanties) + b, name='test_model_errors')\n",
    "                '''getting calibration factors *** Nonlinear'''\n",
    "                # a, b = get_calibration_factors(train_residuals, train_ensemble_uncertanties)\n",
    "                # print(f'a = {a} b = {b}')\n",
    "                # calibrated_train_uncertainties = pd.Series(a * (train_ensemble_uncertanties**((b/2) + 1)), name='train_model_errors')\n",
    "                # calibrated_test_uncertainties = pd.Series(a * (test_ensemble_uncertanties**((b/2) + 1)), name='test_model_errors')\n",
    "\n",
    "\n",
    "                '''\n",
    "                Calculating and plotting performance metrics as outlined in section 2.3 of Tran et al. (https://dx.doi.org/10.1088/2632-2153/ab7e1a)\n",
    "                \n",
    "                Models should be compared in terms of \n",
    "                1st - accuracy (R^2) \n",
    "                2nd - calibration (miscalibration area)\n",
    "                3rd - sharpness\n",
    "                4th - dispersion\n",
    "                '''\n",
    "                #CALIBRATION plots and miscalibration area\n",
    "                #This tells us how 'honest' our uncertainty values are. A perfect calibration plot would mean for a given confidence interval in our prediction\n",
    "                #(say 90%), we can expect with 90% certainty that the true value falls within that confidence interval.\n",
    "                miscalibration_area, calibration_error = make_calibration_plots(model_type, test_predictions_arr, test_labels_arr, test_ensemble_uncertanties, results_saving_folder)\n",
    "                \n",
    "                #SHARPNESS plots and value\n",
    "                #Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\n",
    "                #should a be sharp (i.e. uncertainties should be as small as possible.)\n",
    "                #Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\n",
    "                stdevs = np.array(test_ensemble_uncertanties)/2 #right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\n",
    "                sharpness, dispersion = plot_sharpness_curve(stdevs, results_saving_folder)\n",
    "                #DISPERSION value\n",
    "                #Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \n",
    "                #meaningful UQ, having a large dispersion of uncertainties is valuable. \n",
    "                #Dispersion is calculated using equation 4 of the paper, which is called the coefficient of variation (Cv). #COMMENT High dispersion (Cv) values are better.\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # blank_model_for_plot = SklearnModel('RandomForestRegressor')\n",
    "                # mastml_RVE = Error()\n",
    "\n",
    "                # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "                #                                                         model=blank_model_for_plot, \n",
    "                #                                                         data_type='train', \n",
    "                #                                                         model_errors=pd.Series(train_ensemble_uncertanties) ,\n",
    "                #                                                         model_errors_cal= calibrated_train_uncertainties,\n",
    "                #                                                         residuals= train_residuals, \n",
    "                #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "                #                                                         show_figure=False,\n",
    "                #                                                         well_sampled_number=0.025)\n",
    "                \n",
    "                \n",
    "                # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "                #                                                         model=blank_model_for_plot, \n",
    "                #                                                         data_type='test', \n",
    "                #                                                         model_errors=pd.Series(test_ensemble_uncertanties) ,\n",
    "                #                                                         model_errors_cal= calibrated_test_uncertainties,\n",
    "                #                                                         residuals= test_residuals, \n",
    "                #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "                #                                                         show_figure=False,\n",
    "                #                                                         well_sampled_number=0.025)\n",
    "                \n",
    "                '''using library from Palmer et al. to make an rve plot'''\n",
    "                # train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope = make_RVE_plots(label_to_predict, model_type, test_ensemble_predictions, test_ensemble_uncertanties, test_labels, train_ensemble_predictions, train_ensemble_uncertanties, train_labels, results_saving_folder, num_bins=15)\n",
    "                \n",
    "                '''collecting the performance data from this model'''\n",
    "                # performance_data.append([15, fold_no, train_r2, test_r2, a, b, train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope, miscalibration_area, calibration_error])\n",
    "                performance_data.append([fold_no, train_r2, test_r2, miscalibration_area, calibration_error, sharpness, dispersion])\n",
    "\n",
    "            # columns = ['num bins', 'fold_no', 'train R2', 'test R2',  'a', 'b', 'train_intercept', 'train_slope', 'CAL_train_intercept', 'CAL_train_slope', 'train_intercept', 'test_slope', 'CAL_test_intercept', 'CAL_test_slope', 'miscal_area', 'cal_error']\n",
    "            columns = ['fold_no', 'train R2', 'test R2', 'miscal_area', 'cal_error', 'sharpness', 'dispersion']\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "            for row in performance_data:\n",
    "                df.loc[len(df)] = row\n",
    "            average_row = df.mean()\n",
    "            df = df.append(average_row, ignore_index=True)\n",
    "                \n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models'\n",
    "            df.to_csv(results_saving_folder + f'/{label_to_predict}_{model_type}_{num_models}results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
