{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the code in bagging_models.py for uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forestci is an optional dependency. To install latest forestci compatabilty with scikit-learn>=0.24, run pip install git+git://github.com/scikit-learn-contrib/forest-confidence-interval.git\n",
      "XGBoost is an optional dependency. If you want to use XGBoost models, please manually install xgboost package with pip install xgboost. If have error with finding libxgboost.dylib library, dobrew install libomp. If do not have brew on your system, first do ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" from the Terminal\n",
      "scikit-lego is an optional dependency, enabling use of the LowessRegression model. If you want to use this model, do \"pip install scikit-lego\"\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "'''First, we need to define the path of where to get the dataset, and define other parameters that we will need'''\n",
    "import sys\n",
    "sys.path.append('/Users/jakehirst/Desktop/sfx/sfx_ML_code/sfx_ML/New_Models')\n",
    "\n",
    "from Bagging_models import *\n",
    "from ReCalibration import *\n",
    "from Backward_feature_selection import *\n",
    "import ast\n",
    "\n",
    "model_types = ['ANN', 'RF', 'GPR','ridge']\n",
    "model_types = ['GPR']\n",
    "all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "\n",
    "num_models_list = [20]\n",
    "labels_to_predict = ['impact site x', 'impact site y', 'height']\n",
    "labels_to_predict = ['impact site y', 'height']\n",
    "\n",
    "with_or_without_transformations = 'with'\n",
    "with_or_without_transformations = 'without'\n",
    "\n",
    "Paper2_path = f'/Volumes/Jake_ssd/Paper 2/recalibrations/{with_or_without_transformations}_transformations'\n",
    "\n",
    "if(not os.path.exists(Paper2_path)): os.makedirs(Paper2_path)\n",
    "model_folder = Paper2_path + f'/UQ_bagging_models_{with_or_without_transformations}_transformations'\n",
    "data_folder = Paper2_path + '/5fold_datasets'\n",
    "results_folder = Paper2_path + '/Compare_Code_5_fold_ensemble_results'\n",
    "# hyperparam_folder = Paper2_path + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "hyperparam_folder = f'/Volumes/Jake_ssd/Paper 2/{with_or_without_transformations}_transformations' + f'/bayesian_optimization_{with_or_without_transformations}_transformations'\n",
    "\n",
    "\n",
    "image_folder = '/Users/jakehirst/Desktop/sfx/sfx_ML_data/images_sfx/new_dataset/Visible_cracks'\n",
    "\n",
    "if(with_or_without_transformations == 'with'):\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/dataset/feature_transformations_2023-11-16/height/HEIGHTALL_TRANSFORMED_FEATURES.csv\"\n",
    "    backward_feat_selection_results_folder = '/Volumes/Jake_ssd/Paper 1/Paper_1_results_WITH_feature_engineering/results'\n",
    "else:\n",
    "    # full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 1/Paper_1_results_no_feature_engineering/dataset/New_Crack_Len_FULL_OG_dataframe_2023_11_16.csv\"\n",
    "    full_dataset_pathname = \"/Volumes/Jake_ssd/Paper 2/New_Crack_Len_FULL_OG_dataframe_2024_02_22.csv\"\n",
    "    backward_feat_selection_results_folder = Paper2_path + '/Paper_2_results_WITHOUT_feature_engineering/results' \n",
    "    df = pd.read_csv(full_dataset_pathname, index_col=0)\n",
    "    all_features = df.columns\n",
    "    all_features = all_features.drop(all_labels)\n",
    "    all_features = str(all_features.drop('timestep_init').to_list())\n",
    "    print(all_features)\n",
    " \n",
    "\n",
    "\n",
    "'''Only have to uncomment this if the 5 fold datasets have not been made or need to be remade'''\n",
    "# make_5_fold_datasets(data_folder, full_dataset_pathname, image_folder)\n",
    "\n",
    "\n",
    "print('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using just the basic features\n",
      "using just the basic features\n",
      "{'impact site y': {'GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}, 'height': {'GPR': \"['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\"}}\n"
     ]
    }
   ],
   "source": [
    "'''get the appropriate features that each model will use based on backward feature elimination'''\n",
    "all_features_to_keep = {}\n",
    "\n",
    "min_features = 1 #minimum number of features you want to select from BFS (backward feature selection)\n",
    "max_features = 25 #maximum number of features you want to select from BFS\n",
    "for label in labels_to_predict:\n",
    "    all_features_to_keep[label] = {}\n",
    "    for model_type in model_types:\n",
    "        \n",
    "        if('ALL_TRANSFORMED_FEATURES' in full_dataset_pathname):\n",
    "            print('true')\n",
    "        #TODO use code below if using feature selection\n",
    "            best_features = get_best_features(backward_feat_selection_results_folder, label, model_type, min_features, max_features)\n",
    "            all_features_to_keep[label][model_type] = best_features\n",
    "        \n",
    "        else:\n",
    "            print('using just the basic features')\n",
    "            #TODO use code below if NOT using feature selection\n",
    "            all_features_to_keep[label][model_type] = all_features\n",
    "\n",
    "print(all_features_to_keep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/impact site y/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/height/fold1/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/impact site y/fold2/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/height/fold2/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/impact site y/fold3/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/height/fold3/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/impact site y/fold4/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/height/fold4/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting impact site y using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/impact site y/fold5/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting height using GPR $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "/Volumes/Jake_ssd/Paper 2/recalibrations/without_transformations/5fold_datasets/height/fold5/train_features.csv\n",
      "['init z', 'init y', 'init x', 'max_prop_speed', 'avg_prop_speed', 'dist btw frts', 'crack len', 'linearity', 'max thickness', 'mean thickness', 'median_thickness', 'var_thickness', 'std_thickness', 'thickness_at_init', 'max_kink', 'abs_val_mean_kink', 'mean_kink', 'sum_kink', 'abs_val_sum_kink', 'median_kink', 'std_kink', 'var_kink', 'avg_ori', 'angle_btw']\n",
      "working on model 0\n",
      "working on model 1\n",
      "working on model 2\n",
      "working on model 3\n",
      "working on model 4\n",
      "working on model 5\n",
      "working on model 6\n",
      "working on model 7\n",
      "working on model 8\n",
      "working on model 9\n",
      "working on model 10\n",
      "working on model 11\n",
      "working on model 12\n",
      "working on model 13\n",
      "working on model 14\n",
      "working on model 15\n",
      "working on model 16\n",
      "working on model 17\n",
      "working on model 18\n",
      "working on model 19\n"
     ]
    }
   ],
   "source": [
    "'''Now we will make all of the bagging models'''\n",
    "for fold_no in range(1,6):\n",
    "    for model_type in model_types:\n",
    "        for label_to_predict in labels_to_predict:\n",
    "            for num_models in num_models_list:\n",
    "                \n",
    "                print(f'\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\ Predicting {label_to_predict} using {model_type} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n')\n",
    "                \n",
    "                all_labels = ['height', 'phi', 'theta', \n",
    "                            'impact site x', 'impact site y', 'impact site z', \n",
    "                            'impact site r', 'impact site phi', 'impact site theta']\n",
    "\n",
    "\n",
    "                print(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv')\n",
    "                training_features = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_features.csv').reset_index(drop=True)\n",
    "                training_labels = pd.read_csv(f'{data_folder}/{label_to_predict}/fold{fold_no}/train_labels.csv').reset_index(drop=True)\n",
    "\n",
    "                model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                if(not os.path.exists(model_saving_folder)):\n",
    "                    os.makedirs(model_saving_folder)\n",
    "                    \n",
    "                results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                if(not os.path.exists(results_saving_folder)):\n",
    "                    os.makedirs(results_saving_folder)\n",
    "                # make_dirs(model_saving_folder)\n",
    "                # make_dirs(results_saving_folder)\n",
    "\n",
    "                '''TODO gotta find out what features to use for each label before testing on new dataset'''\n",
    "                features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "                print(features_to_keep)\n",
    "                make_linear_regression_models_for_ensemble(training_features, training_labels, model_saving_folder, label_to_predict, num_models, features_to_keep, hyperparam_folder, model_type=model_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE = GPR\n",
      "LABEL = impact site y\n",
      "fold 1\n",
      "Calibration error = 3.75\n",
      "fold 2\n",
      "Calibration error = 3.61\n",
      "fold 3\n",
      "Calibration error = 3.15\n",
      "fold 4\n",
      "Calibration error = 3.80\n",
      "fold 5\n",
      "Calibration error = 4.46\n",
      "LABEL = height\n",
      "fold 1\n",
      "Calibration error = 3.74\n",
      "fold 2\n",
      "Calibration error = 4.54\n",
      "fold 3\n",
      "Calibration error = 3.95\n",
      "fold 4\n",
      "Calibration error = 5.16\n",
      "fold 5\n",
      "Calibration error = 4.51\n"
     ]
    }
   ],
   "source": [
    "'''Now we will evaluate the performance of the bagging models'''\n",
    "\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'MODEL TYPE = {model_type}')\n",
    "    for label_to_predict in labels_to_predict:\n",
    "        print(f'LABEL = {label_to_predict}')\n",
    "        for num_models in num_models_list:\n",
    "            performance_data = []\n",
    "            for fold_no in range(1,6):\n",
    "                print(f'fold {fold_no}')\n",
    "\n",
    "                #defining folders to get the models and to store the results\n",
    "                model_saving_folder = f'{model_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models/fold_{fold_no}'\n",
    "                \n",
    "                #defining folders where the datasets are coming from (5-fold cv)\n",
    "                test_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_features.csv'\n",
    "                test_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/test_labels.csv'\n",
    "                train_features_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_features.csv'\n",
    "                train_labels_path = Paper2_path + f'/5fold_datasets/{label_to_predict}/fold{fold_no}/train_labels.csv'\n",
    "\n",
    "                #defining the features that each model used (since they vary with each model)\n",
    "                features_to_keep = ast.literal_eval(all_features_to_keep[label_to_predict][model_type])\n",
    "                \n",
    "                #predicting the test and train sets with the bagging models\n",
    "                test_r2, test_ensemble_predictions, test_ensemble_uncertanties, test_labels = Get_predictions_and_uncertainty_with_bagging(test_features_path, test_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "                train_r2, train_ensemble_predictions, train_ensemble_uncertanties, train_labels = Get_predictions_and_uncertainty_with_bagging(train_features_path, train_labels_path, model_saving_folder, results_saving_folder, features_to_keep, label_to_predict, model_type)\n",
    "\n",
    "                #defining the residual errors of the predictions\n",
    "                train_labels_arr = train_labels\n",
    "                train_predictions_arr = np.array(train_ensemble_predictions)\n",
    "                test_labels_arr = test_labels\n",
    "                test_predictions_arr = np.array(test_ensemble_predictions)\n",
    "                train_residuals = pd.Series(np.abs(train_labels_arr - train_predictions_arr))\n",
    "                test_residuals = pd.Series(np.abs(test_labels_arr - test_predictions_arr))\n",
    "\n",
    "                a = 0\n",
    "                b = 0\n",
    "                '''getting calibration factors *** linear'''\n",
    "                # cf = CorrectionFactors(train_residuals, pd.Series(train_ensemble_uncertanties))\n",
    "                # a, b = cf.nll()\n",
    "                # print(f'a = {a} b = {b}')\n",
    "                # calibrated_train_uncertainties = pd.Series(a * np.array(train_ensemble_uncertanties) + b, name='train_model_errors')\n",
    "                # calibrated_test_uncertainties = pd.Series(a * np.array(test_ensemble_uncertanties) + b, name='test_model_errors')\n",
    "                '''getting calibration factors *** Nonlinear'''\n",
    "                # a, b = get_calibration_factors(train_residuals, train_ensemble_uncertanties)\n",
    "                # print(f'a = {a} b = {b}')\n",
    "                # calibrated_train_uncertainties = pd.Series(a * (train_ensemble_uncertanties**((b/2) + 1)), name='train_model_errors')\n",
    "                # calibrated_test_uncertainties = pd.Series(a * (test_ensemble_uncertanties**((b/2) + 1)), name='test_model_errors')\n",
    "\n",
    "\n",
    "                '''\n",
    "                Calculating and plotting performance metrics as outlined in section 2.3 of Tran et al. (https://dx.doi.org/10.1088/2632-2153/ab7e1a)\n",
    "                \n",
    "                Models should be compared in terms of \n",
    "                1st - accuracy (R^2) \n",
    "                2nd - calibration (miscalibration area)\n",
    "                3rd - sharpness\n",
    "                4th - dispersion\n",
    "                '''\n",
    "                #CALIBRATION plots and miscalibration area\n",
    "                #This tells us how 'honest' our uncertainty values are. A perfect calibration plot would mean for a given confidence interval in our prediction\n",
    "                #(say 90%), we can expect with 90% certainty that the true value falls within that confidence interval.\n",
    "                miscalibration_area, calibration_error = make_calibration_plots(model_type, test_predictions_arr, test_labels_arr, test_ensemble_uncertanties, results_saving_folder)\n",
    "                \n",
    "                #SHARPNESS plots and value\n",
    "                #Models can be calibrated, but all have very dull uncertainty values (they all have large uncertainties). To ensure UQ is meaningful, models\n",
    "                #should a be sharp (i.e. uncertainties should be as small as possible.)\n",
    "                #Sharpness is essentially calculated as the average of predicted standard deviations. #COMMENT Low sharpness values are better.\n",
    "                stdevs = np.array(test_ensemble_uncertanties)/2 #right now, i multiply the stds by 2 to make it look better in parity plots... but this needs the raw std.\n",
    "                sharpness, dispersion = plot_sharpness_curve(stdevs, results_saving_folder)\n",
    "                #DISPERSION value\n",
    "                #Models can be calibrated and sharp, but even so, if they are all similar uncertainties, then this does not tell us much. To ensure more \n",
    "                #meaningful UQ, having a large dispersion of uncertainties is valuable. \n",
    "                #Dispersion is calculated using equation 4 of the paper, which is called the coefficient of variation (Cv). #COMMENT High dispersion (Cv) values are better.\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # blank_model_for_plot = SklearnModel('RandomForestRegressor')\n",
    "                # mastml_RVE = Error()\n",
    "\n",
    "                # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "                #                                                         model=blank_model_for_plot, \n",
    "                #                                                         data_type='train', \n",
    "                #                                                         model_errors=pd.Series(train_ensemble_uncertanties) ,\n",
    "                #                                                         model_errors_cal= calibrated_train_uncertainties,\n",
    "                #                                                         residuals= train_residuals, \n",
    "                #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "                #                                                         show_figure=False,\n",
    "                #                                                         well_sampled_number=0.025)\n",
    "                \n",
    "                \n",
    "                # mastml_RVE.plot_real_vs_predicted_error_uncal_cal_overlay(savepath=results_saving_folder, \n",
    "                #                                                         model=blank_model_for_plot, \n",
    "                #                                                         data_type='test', \n",
    "                #                                                         model_errors=pd.Series(test_ensemble_uncertanties) ,\n",
    "                #                                                         model_errors_cal= calibrated_test_uncertainties,\n",
    "                #                                                         residuals= test_residuals, \n",
    "                #                                                         dataset_stdev=np.std(train_labels.to_numpy()), \n",
    "                #                                                         show_figure=False,\n",
    "                #                                                         well_sampled_number=0.025)\n",
    "                \n",
    "                '''using library from Palmer et al. to make an rve plot'''\n",
    "                # train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope = make_RVE_plots(label_to_predict, model_type, test_ensemble_predictions, test_ensemble_uncertanties, test_labels, train_ensemble_predictions, train_ensemble_uncertanties, train_labels, results_saving_folder, num_bins=15)\n",
    "                \n",
    "                '''collecting the performance data from this model'''\n",
    "                # performance_data.append([15, fold_no, train_r2, test_r2, a, b, train_intercept, train_slope, CAL_train_intercept, CAL_train_slope, train_intercept, test_slope, CAL_test_intercept, CAL_test_slope, miscalibration_area, calibration_error])\n",
    "                performance_data.append([fold_no, train_r2, test_r2, miscalibration_area, calibration_error, sharpness, dispersion])\n",
    "\n",
    "            # columns = ['num bins', 'fold_no', 'train R2', 'test R2',  'a', 'b', 'train_intercept', 'train_slope', 'CAL_train_intercept', 'CAL_train_slope', 'train_intercept', 'test_slope', 'CAL_test_intercept', 'CAL_test_slope', 'miscal_area', 'cal_error']\n",
    "            columns = ['fold_no', 'train R2', 'test R2', 'miscal_area', 'cal_error', 'sharpness', 'dispersion']\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "            for row in performance_data:\n",
    "                df.loc[len(df)] = row\n",
    "            average_row = df.mean()\n",
    "            df = df.append(average_row, ignore_index=True)\n",
    "                \n",
    "            results_saving_folder = f'{results_folder}/{label_to_predict}/{model_type}/{num_models}_models'\n",
    "            df.to_csv(results_saving_folder + f'/{label_to_predict}_{model_type}_{num_models}results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
